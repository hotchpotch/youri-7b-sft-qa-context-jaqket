# youri-7b-sft-qa-context-jaqket

[youri-7b-instruction](https://huggingface.co/rinna/youri-7b-instruction) を使って SFT で Q&amp;A + コンテキスト形式を学習するコード各種や　サンプルコード。詳しくは以下を参照ください。

- [Q&A + RAG に特化したLLMをSFTで学習させ4bit量子化モデルを作り、GPT3.5以上の性能を7Bモデルで達成する](https://secon.dev/entry/2023/12/15/080000-qa-rag-llm-sft/)
- [学習コードや評価コード](https://github.com/hotchpotch/youri-7b-sft-qa-context-jaqket/)
- [hotchpotch/youri-7b-sft-qa-context-jaqket-gptq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-gptq)
  - AutoGPTQ で量子化したモデル
- [hotchpotch/youri-7b-sft-qa-context-jaqket-awq](https://huggingface.co/hotchpotch/youri-7b-sft-qa-context-jaqket-awq)
  - AutoAWQ で量子化したモデル
